import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
from wordcloud import WordCloud

st.set_page_config(
     page_title="An√°lise Explorat√≥ria",
     page_icon="üåµ",
     layout="wide",
     initial_sidebar_state="expanded",
     menu_items={}
 )

st.experimental_singleton.clear()

## Abertura de arquivo e cria√ß√£o do dataframe:
@st.experimental_singleton
def carrega_base(path):
    data = pd.read_csv(path, sep=",", low_memory=True)
    data.drop_duplicates(subset='tweet_id', keep='first', inplace=True)
    return data

df = carrega_base('data/corpus_labeled/iguais/bases_tcc/03_geracao_carcteristicas_base.csv')
#dfA1 = carrega_base("data/corpus_labeled/rotula√ßao1[anisiofilho].csv")
#dfB1 = carrega_base("data/corpus_labeled/rotula√ßao1[debora].csv")
dfA2 = carrega_base("data/corpus_labeled/rotula√ßao2[anisiofilho].csv")
dfB2 = carrega_base("data/corpus_labeled/rotula√ßao2[andre].csv")

## Fun√ß√µes
### M√≥dulos
#### Ajustador de turnos
def ajusta_turno(linha):
    return linha.capitalize()

##### Ajustador de usu√°rios
def ajusta_usuarios(linha):
    if linha == 'invalid_user':
        return 'Usu√°rio Inv√°lido'
    else:
        return linha

#### Ajustador de estados
def ajusta_estados(linha):
    if linha == 'invalidstate':
        return 'Inv√°lido (Usu√°rio n√£o informou sua localiza√ß√£o)'
    elif linha == 'notbrazilstate':
        return 'Exterior (Fora do Brasil)'
    elif linha == 'statenotdefined':
        return 'Brasil (Localiza√ß√£o definida apenas como Brasil)'
    else:
        return linha

#### Ajustador de regi√µes
def ajusta_regioes(linha):
    if linha == 'invalidregion':
        return 'Inv√°lido<br>(Usu√°rio n√£o informou sua localiza√ß√£o)'
    elif linha == 'notbrazilregion':
        return 'Exterior<br>(Fora do Brasil)'
    elif linha == 'regionnotdefined':
        return 'Brasil<br>(Localiza√ß√£o definida apenas como Brasil)'
    else:
        return linha

#### Ajustador de r√≥tulos
def ajusta_rotulo(linha):
    if linha == -1:
        return 'Not√≠cia Verdadeira'
    elif linha == 0:
        return 'Opini√£o'
    elif linha == 1:
        return 'Not√≠cia Falsa'

def gera_frequencia_palavras1(dicionario_frequencia, dataframe, coluna):
    for index, row in dataframe.iterrows():
        row[coluna] = eval(row[coluna])
        for palavra in row[coluna]:
            if palavra not in dicionario_frequencia.keys():
                dicionario_frequencia[palavra] = 1
            else:
                dicionario_frequencia[palavra] += 1
    
    dicionario_frequencia = dict(sorted(dicionario_frequencia.items(), key=lambda item: item[1]))
    
    return dicionario_frequencia

### Contador de frequ√™ncia de palavras:
def gera_frequencia_palavras2(dicionario_frequencia, dataframe, coluna, rotulo):
    for index, row in dataframe.iterrows():
        if row.label_A == rotulo:
            row[coluna] = eval(row[coluna])
            for palavra in row[coluna]:
                
                if palavra not in dicionario_frequencia.keys():
                    dicionario_frequencia[palavra] = 1
                else:
                    dicionario_frequencia[palavra] += 1
    
    dicionario_frequencia = dict(sorted(dicionario_frequencia.items(), key=lambda item: item[1]))
    
    return dicionario_frequencia

### Gerador de nuvem de palavras
def gera_nuvem_palavras(dicionario_frequencia):
    wordcloud = WordCloud(#font_path=None, 
                        width=800, 
                        height=400, 
                        margin=2, 
                        ranks_only=None, 
                        prefer_horizontal=0.9, 
                        mask=None, 
                        scale=1, 
                        color_func=None, 
                        max_words=500, 
                        min_font_size=4, 
                        stopwords=None, 
                        random_state=None, 
                        background_color='white', 
                        max_font_size=None, 
                        font_step=1, 
                        mode='RGB', 
                        relative_scaling='auto', 
                        regexp=None, 
                        collocations=True, 
                        colormap='hsv', 
                        normalize_plurals=True, 
                        contour_width=0, 
                        contour_color='black', 
                        repeat=False, 
                        include_numbers=False, 
                        min_word_length=0, 
                        collocation_threshold=30).generate_from_frequencies(dicionario_frequencia)
    
    fig, ax = plt.subplots(figsize=(16,8))
    ax.imshow(wordcloud, interpolation='bilinear')       
    ax.set_axis_off()
    
    return fig, wordcloud

## MAIN
st.header("An√°lise Explorat√≥ria da Base de Dados")

st.write('O estudo de Filho et al. (2021) coletou uma base com um total de 10573 tweets, dois quais 1963 tweets foram rotulados e publicados, sendo: 320 tweets pertencentes √† classe news; 275 pertencentes √† classe fake e; 705 contidos na classe opinion. Os rotuladores obtiveram um √≠ndice Kappa = 0.47, que √© considerado uma concord√¢ncia moderada de avalia√ß√£o. Neste trabalho, considerando o mesmo protocolo de anota√ß√£o, foram acrescentados mais 2300 tweets ao corpus apresentado em Filho et al. (2021). Os rotuladores dos novos exemplos do corpus alcan√ßaram um √≠ndice Kappa = 0.52, tamb√©m considerado moderado.')
st.write('O gr√°fico abaixo apresenta a distribui√ß√£o dos r√≥tulos considerando a base de dados ampliada (Covid-19 Text Dataset - CTD).')

df_dist_rotuloA2 = dfA2['label'].value_counts().reset_index()
df_dist_rotuloA2['Rotulador'] = 'A2'
df_dist_rotuloA2.rename(columns={'index':'R√≥tulo', 'label':'Quantidade'}, inplace=True)
df_dist_rotuloA2['R√≥tulo'] = df_dist_rotuloA2['R√≥tulo'].map(ajusta_rotulo)

df_dist_rotuloB2 = dfB2['label'].value_counts().reset_index()
df_dist_rotuloB2['Rotulador'] = 'B2'
df_dist_rotuloB2.rename(columns={'index':'R√≥tulo', 'label':'Quantidade'}, inplace=True)
df_dist_rotuloB2['R√≥tulo'] = df_dist_rotuloB2['R√≥tulo'].map(ajusta_rotulo)

df_dist_rotuloA2B2 = pd.concat([df_dist_rotuloA2, df_dist_rotuloB2])

fig9 = px.bar(df_dist_rotuloA2B2, 
            x='R√≥tulo', 
            y='Quantidade', 
            color='Rotulador',
            color_discrete_map={
                                    #'A1':'tomato',
                                    'A2':'yellowgreen',
                                    'B2':'steelblue',
                                },
            barmode='group',
            title='Quantidade de cada r√≥tulo por rotulador', 
            text_auto=True,
            width=1000,
            height=600,
            )
st.plotly_chart(fig9, use_container_width=True)

st.write('Foram incorporados a base de dados j√° existente apenas os r√≥tulos que obtiveram concord√¢ncia entre os dois anotadores dos novos exemplos. Sendo assim, o CTD foi cont√©m 40 um total de 3600 exemplos, distribu√≠dos entre as classes da seguinte forma: 487 para a classe fake; 714 contidos na classe new e; 2399 pertencentes √† classe opinion.')


st.write('Com a finalidade de encontrar um padr√£o entre as publica√ß√µes de not√≠cias falsas, foram realizadas an√°lises relacionadas a caracter√≠sticas como hor√°rio da postagem, localiza√ß√£o e IDs dos usu√°rios. Vale ressaltar que as pr√≥ximas an√°lises consideram sempre a base de dados ampliada (CTD).')

st.write('Os estudos relacionados aos hor√°rio de postagem consideraram as seguintes divis√µes por turno: Manh√£ compreende o per√≠odo das 00h00min √†s 11h59min; Tarde, s√£o as postagens realizadas das 12h00min √†s 17h59min e; Noite compreende o intervalo de horas das 18h00min √†s 23h59min. As postagens relacionadas √† Covid-19 ocorrem, em geral, no per√≠odo da noite, correspondendo a cerca de 86,11% do total de tweets, enquanto que o turno da manh√£ apresenta o menor per√≠odo de engajamento, com aproximadamente 4,19\% do total de postagens da base. A distribui√ß√£o das classes considerando as postagens por turno √© apresentada no gr√°fico abaixo.')
df_turnos_rotulos = df[['time_shift', 'label_A']].value_counts().reset_index()
df_turnos_rotulos.rename(columns={'time_shift':'Turno','label_A':'R√≥tulo', 0:'Quantidade'}, inplace=True)
df_turnos_rotulos.sort_values(by='Quantidade', ascending=False, inplace=True)
df_turnos_rotulos['Turno'] = df_turnos_rotulos['Turno'].map(ajusta_turno) 
df_turnos_rotulos['R√≥tulo'] = df_turnos_rotulos['R√≥tulo'].map(ajusta_rotulo)

fig5 = px.bar(df_turnos_rotulos, 
            x='R√≥tulo', 
            y='Quantidade', 
            color='Turno',
            color_discrete_map={
                                    'Manh√£':'tomato',
                                    'Tarde':'yellowgreen',
                                    'Noite':'steelblue',
                                },
            barmode='group',
            title='Quantidade de postagens de cada r√≥tulo por turno',
            text_auto=True,
            width=1000,
            height=600,
            )
st.plotly_chart(fig5, use_container_width=True)
st.write('O gr√°fico acima demonstra que todas as classes apresentam uma distribui√ß√£o similar, em que a maioria dos tweets ocorreram no per√≠odo da noite, seguidos pela tarde e por fim o turno da manh√£. Para a classe fake, os tweets nortunos correspondem a 87,27\% dos 487 exemplos desta classe. Quando observamos a classe news vemos que dos seus 714 tweets, quase 80\% foram publicados no turno da noite. Para a classe opinion 87,83\% dos seus tweets foram postados a noite. Portanto, n√£o foi poss√≠vel identificar um hor√°rio em espec√≠fico em que ocorre maior incid√™ncia de postagens de not√≠cias falsas.')

st.write('Para a an√°lise dos dados separando as postagens por Estados o processamento da localiza√ß√£o dos usu√°rios se deu da seguinte forma: primeiro converteu-se o texto em letras min√∫sculas, foram substitu√≠das as siglas dos Estados para seu nome completo e tamb√©m removeu-se os s√≠mbolos (acentos, pontua√ß√µes e demais simbologias da l√≠ngua). Por fim, utilizou-se a biblioteca Geopy para encontrar a localiza√ß√£o mais precisa poss√≠vel.')

st.write('A classifica√ß√£o se deu da seguinte forma:')
st.write('\t1. Os Estados que se encaixaram como sendo pertencentes ao Brasil, foram classificados com seus respectivos nomes;')
st.write('\t2. As localiza√ß√µes que s√≥ apresentavam o Brasil como refer√™ncia, mas n√£o apresentavam uma cidade ou Estado identific√°vel foram identificados na categoria ‚Äòregionnotdefined‚Äô;')
st.write('\t3. As localiza√ß√µes que n√£o identificaram o Brasil como pa√≠s de postagem, por√©m apresentavam algum outro pa√≠s, foram categorizados como ‚Äònotbrazilstate‚Äô;')
st.write('\t4. As localiza√ß√µes classificadas como ‚Äòinvalidlocation‚Äô ou com conte√∫do que n√£o se encaixam nas categorias anteriores, como localiza√ß√µes com piadas ou conte√∫do incompreens√≠vel foram classificadas como ‚Äòinvalidstate‚Äô.')

st.write('Ap√≥s pr√©-processamento das localiza√ß√µes, foram encontrados os seguintes resultados: aproximadamente 47,47\% do total das postagens correspondem a Estados inv√°lidos, enquanto que o Estado de S√£o Paulo se sobressai com cerca de 13,19\% do total. Estados n√£o pertencentes ao Brasil e Estados n√£o definidos aparecem com cerca de 10\% e 7,44\% do total, respectivamente, na sequ√™ncia temos o Rio de Janeiro, com postagens correspondentes a aproximadamente 6,08\%. Os demais Estados possuem menos de 5% dos exemplos rotulados nesta base de dados. O gr√°fico abaixo apresenta os resultados.')
df_estados_grafico = df.state_location.value_counts().reset_index()
df_estados_grafico.rename(columns={'index':'Estado','state_location':'Quantidade'}, inplace=True)
df_estados_grafico.sort_values(by='Quantidade', ascending=True, inplace=True)
df_estados_grafico['Estado'] = df_estados_grafico['Estado'].map(ajusta_estados)

fig3 = px.bar(df_estados_grafico, 
            y='Estado', 
            x='Quantidade', 
            title='Quantidade de postagens por Unidade Federativa', 
            text_auto=True, 
            orientation='h',
            width=1000,
            height=600,
            )
st.plotly_chart(fig3, use_container_width=True)

st.write('A an√°lise dos dados separando as postagens por regi√£o se deu de forma similizar ao processo utilizado para definir os Estados, a diferen√ßa consiste na defini√ß√£o de regi√£o, em que utilizou-se uma lista de Estados do Brasil para, a partir do Estado, identificarmos sua respectiva regi√£o. A classifica√ß√£o se deu da seguinte forma:')

st.write('\t1. As regi√µes identificadas como pertencentes ao Brasil, foram classificados com seus respectivos nomes;')
st.write('\t2. As localiza√ß√µes que s√≥ apresentavam o Brasil como localiza√ß√£o, mas n√£o apresentavam uma regi√£o detect√°vel foram colocados na categoria ‚Äòregionnotdefined‚Äô;')
st.write('\t3. As localiza√ß√µes que n√£o identificaram o pa√≠s como Brasil, por√©m como sendo algum outro pa√≠s foram categorizados como ‚Äònotbrazilregion‚Äô;')
st.write('\t4. As localiza√ß√µes classificadas como ‚Äòinvalidlocation‚Äô ou com conte√∫do que n√£o se encaixa nas categorias anteriores, como localiza√ß√µes com piadas ou conte√∫do incom- preens√≠vel, foram classificadas como ‚Äòinvalidregion‚Äô.')

st.write('Dessa forma, cerca de 47,47\% do total da base corresponde a regi√µes inv√°lidas, enquanto que a regi√£o Sudeste se sobressai √†s demais com o aproximado de 22,92\% da base de dados, regi√µes n√£o pertencentes ao Brasil aparecem em aproximadamente 10\% da base, enquanto que regi√µes indefinidas correspondem ao valor pr√≥ximo de 7,44\% da base. As regi√µes Sul e Nordeste, aparecem com cerca de 5,64\% e 4,14\% dos tweets, respectivamente. As regi√µes Norte e Centro-Oeste contam com, sequencialmente, 1,5\% e aproximadamente 0,88\% do total de exemplos da base de dados.')
df_regioes_rotulos = df[['region_location', 'label_A']].value_counts().reset_index()
df_regioes_rotulos.rename(columns={'region_location':'Regi√£o','label_A':'R√≥tulo', 0:'Quantidade'}, inplace=True)
df_regioes_rotulos.sort_values(by='Quantidade', ascending=True, inplace=True)
df_regioes_rotulos['R√≥tulo'] = df_regioes_rotulos['R√≥tulo'].map(ajusta_rotulo) 
df_regioes_rotulos['Regi√£o'] = df_regioes_rotulos['Regi√£o'].map(ajusta_regioes)

fig6 = px.bar(df_regioes_rotulos, 
            y='Regi√£o', 
            x='Quantidade', 
            color='R√≥tulo',
            color_discrete_map={
                                    'Not√≠cia Falsa':'tomato',
                                    'Not√≠cia Verdadeira':'yellowgreen',
                                    'Opini√£o':'steelblue',
                                },
            barmode='group',
            title='Quantidade de postagens de cada r√≥tulo por turno', 
            text_auto=True,
            orientation='h',
            width=1000,
            height=600,
            )
st.plotly_chart(fig6, use_container_width=True)
st.write('O gr√°fico acima detalha a distribui√ß√£o das classes sob o ponto de vista das regi√µes. Neste ponto, um padr√£o √© identificado, pois 22\% das publica√ß√µes de pessoas que n√£o adicionam suas localiza√ß√µes (invalidregion) pertencem √† classe fake. Quanto as demais localiza√ß√µes, tem-se entre 9\% e 13\% das postagens contendo not√≠cias falsas.')
st.write('Quanto as not√≠cias verdadeiras, tem-se que 24,25\% dos tweets publicados com a localiza√ß√£o ‚Äòregionnotdefined‚Äô s√£o da classe news, seguido das publica√ß√µes da regi√£o Nordeste, que s√£o not√≠cias verdadeiras em 23,49\% dos tu√≠tes desta regi√£o. Ao analisar as postagens do Sul, tem-se not√≠cias verdadeiras em 22,66\% das postagens. As demais regi√µes apresentam um percentual inferior a 20\% de tu√≠tes com not√≠cias verdadeiras. Por fim, √© poss√≠vel observar que a classe opinion √© superior em todas as regi√µes, devido a natureza desbalanceada da base de dados coletada.')


st.write('A an√°lise da base de dados separando as postagens pelos nomes de usu√°rios foi feita recuperando-se o ‚Äòuser_screen_name‚Äô a partir do id de cada tweet, esse processo foi feito utilizando-se um crawler/rob√¥ e a biblioteca Tweepy. Os dados recuperados foram armazenados na base de dados, o processamento foi feito analisando quais usu√°rios possuem mais postagens dentro da base e depois disso foram escolhidos os dez usu√°rios que mais postaram para as an√°lises.')
df_usuarios_grafico = df.user_screen_name.value_counts().reset_index()[0:10]
df_usuarios_grafico.rename(columns={'index':'Nome de usu√°rio','user_screen_name':'Quantidade'}, inplace=True)
df_usuarios_grafico.sort_values(by='Quantidade', ascending=True, inplace=True)
df_usuarios_grafico['Nome de usu√°rio'] = df_usuarios_grafico['Nome de usu√°rio'].map(ajusta_usuarios)
lista_usuarios_top = df_usuarios_grafico['Nome de usu√°rio'].to_list()
lista_usuarios_top.append('invalid_user')

df_usuario_rotulos = df[['user_screen_name', 'label_A']].value_counts().reset_index()
df_usuario_rotulos = df_usuario_rotulos[df_usuario_rotulos['user_screen_name'].isin(lista_usuarios_top)]
df_usuario_rotulos.rename(columns={'user_screen_name':'Nome de usu√°rio','label_A':'R√≥tulo', 0:'Quantidade'}, inplace=True)
df_usuario_rotulos.sort_values(by=['Quantidade'], ascending=True, inplace=True)
df_usuario_rotulos['R√≥tulo'] = df_usuario_rotulos['R√≥tulo'].map(ajusta_rotulo)
df_usuario_rotulos['Nome de usu√°rio'] = df_usuario_rotulos['Nome de usu√°rio'].map(ajusta_usuarios)

fig7 = px.bar(df_usuario_rotulos, 
            y='Nome de usu√°rio', 
            x='Quantidade', 
            color='R√≥tulo',
            color_discrete_map={
                                    'Not√≠cia Falsa':'tomato',
                                    'Not√≠cia Verdadeira':'yellowgreen',
                                    'Opini√£o':'steelblue',
                                },
            barmode='group',
            title='Quantidade de postagens de cada r√≥tulo por nome de usu√°rio', 
            text_auto=True,
            orientation='h',
            width=1000,
            height=600,
            )
st.plotly_chart(fig7, use_container_width=True)
st.write('Ao considerarmos a separa√ß√£o dos tweets de cada usu√°rio por classe, foi poss√≠vel observar que alguns usu√°rios postaram somente not√≠cias verdadeiras, como exemplo tem-se o perfil do Instituto Butantan (butantanoficial). Tamb√©m foi poss√≠vel perceber que algumas contas postaram somente tweets considerados opini√µes. Por√©m, ao analisarmos a classe fake, n√£o foi poss√≠vel observar nenhuma conta que publicou 100\% de not√≠cias falsas. Algumas contas apresentam forte tend√™ncia para publica√ß√£o de fake news, mas nem todos os tweets publicados eram not√≠cias falsas. Esse √© um fator importante de an√°lise, pois alguns usu√°rios podem variar entre not√≠cias verdadeiras e falsas com a finalidade de ganhar a confian√ßa dos demais.')

st.write('Com o intuito de analisar os termos com maior frequ√™ncia na base de dados, foi gerada uma nuvem de palavras, conforme a figura 6, que destaca as palavras "vacina", "covid", "eficacia"e "brasil", como as mais frequentes no corpus.')
lemmatization_dict = dict()
lemmatization_dict = gera_frequencia_palavras1(lemmatization_dict, df, 'tweet_text_lemmatization')
lemmatization_fig, lemmatization_wordcloud = gera_nuvem_palavras(lemmatization_dict)
st.pyplot(lemmatization_fig)

st.write('Tamb√©m foram analisadas as palavras mais frequentes de acordo com cada classe da base de dados. A figura abaixo realiza uma compara√ß√£o entre as palavras mais relevantes tanto para a classe fake, figura a, quanto para a classe news, figura b e tamb√©m para a classe opinion, figura c.')

coluna_lemmatization_fake, coluna_lemmatization_news, coluna_lemmatization_opinion = st.columns(3)
with coluna_lemmatization_fake:
    ##### Fake News
    st.caption('a')
    lemmatization_fake_dict = dict()
    lemmatization_fake_dict = gera_frequencia_palavras2(lemmatization_fake_dict, df, 'tweet_text_lemmatization', 1)

    lemmatization_fake_fig, lemmatization_fake_wordcloud = gera_nuvem_palavras(lemmatization_fake_dict)
    st.pyplot(lemmatization_fake_fig)

with coluna_lemmatization_news:
    ##### News
    st.caption('b')
    lemmatization_news_dict = dict()
    lemmatization_news_dict = gera_frequencia_palavras2(lemmatization_news_dict, df, 'tweet_text_lemmatization', -1)

    lemmatization_news_fig, lemmatization_news_wordcloud = gera_nuvem_palavras(lemmatization_news_dict)
    st.pyplot(lemmatization_news_fig)

with coluna_lemmatization_opinion:
    ##### Opinion
    st.caption('c')
    lemmatization_opinion_dict = dict()
    lemmatization_opinion_dict = gera_frequencia_palavras2(lemmatization_opinion_dict, df, 'tweet_text_lemmatization', -1)

    lemmatization_opinion_fig, lemmatization_opinion_wordcloud = gera_nuvem_palavras(lemmatization_opinion_dict)
    st.pyplot(lemmatization_opinion_fig)

st.write('Observando os resultados para a classe fake, √© poss√≠vel visualizar os termos "eficacia", "brasil", "vacina", "poder", "covid", "dose", "dado", "milhoes" e "contra", como os mais relevantes. Ao analisar os termos da classe news, as palavras com maior relev√¢ncia s√£o "vacina", "covid", "dose", "china", "eficacia", "fazer", "tomar", "doria", "querer"e "usar". J√° para a classe opinon notamos palavras como "covid", "vacina", "eficacia", "milhoes", "brasil", "chin√™s", "dia", "brasil", "dose", "governo", "contra"e "anvisa". √â poss√≠vel perceber que alguns termos s√£o apontados como muito frequentes tanto na classe de not√≠cias verdadeiras quanto para as not√≠cias falsas e tamb√©m na classe de opini√£o. Nesse sentido, um ponto importante de an√°lise est√° relacionado ao comportamento j√° observado quanto aos usu√°rios que costumam publicar not√≠cias falsas, pois estes tamb√©m realizam publica√ß√µes de not√≠cias verdadeiras ou opini√µes. Esses fatores s√£o indicadores da dificuldade que os usu√°rios de redes sociais podem encontrar para minerar o que seria uma not√≠cia verdadeira ou falsa e especificar o que √© apenas opini√£o.')

st.experimental_singleton.clear()