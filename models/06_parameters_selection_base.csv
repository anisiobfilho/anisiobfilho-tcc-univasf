classifier,oversampling,undersampling,parameters_random,f1_score_random,classification_report_random,best_params_random,parameters_grid,f1_score_grid,classification_report_grid,best_params_grid
randomforest,False,True,"{'randomforest__min_samples_leaf': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59], 'randomforest__max_features': [0.1, 0.30000000000000004, 0.5000000000000001, 0.7000000000000001, 0.9000000000000001], 'randomforest__max_depth': [6, 8, 10, 12, 14, 16, 18], 'randomforest__n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190], 'randomforest__random_state': [1]}",[0.49363868 0.70657673 0.26595745],"              precision    recall  f1-score   support

        news       0.84      0.35      0.49       556
     opinion       0.59      0.89      0.71       472
        fake       0.18      0.48      0.27        52

    accuracy                           0.59      1080
   macro avg       0.54      0.57      0.49      1080
weighted avg       0.70      0.59      0.58      1080
","{'randomforest__random_state': 1, 'randomforest__n_estimators': 160, 'randomforest__min_samples_leaf': 1, 'randomforest__max_features': 0.1, 'randomforest__max_depth': 6}","{'randomforest__min_samples_leaf': [1, 2], 'randomforest__max_features': [0.1, 0.2], 'randomforest__max_depth': [4, 5, 6, 7], 'randomforest__n_estimators': [150, 155, 160, 165], 'randomforest__random_state': [1]}",[0.50580645 0.71005917 0.24752475],"              precision    recall  f1-score   support

        news       0.85      0.36      0.51       545
     opinion       0.59      0.90      0.71       469
        fake       0.18      0.38      0.25        66

    accuracy                           0.59      1080
   macro avg       0.54      0.54      0.49      1080
weighted avg       0.70      0.59      0.58      1080
","{'randomforest__max_depth': 7, 'randomforest__max_features': 0.1, 'randomforest__min_samples_leaf': 1, 'randomforest__n_estimators': 160, 'randomforest__random_state': 1}"
randomforest,False,False,"{'randomforest__min_samples_leaf': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59], 'randomforest__max_features': [0.1, 0.30000000000000004, 0.5000000000000001, 0.7000000000000001, 0.9000000000000001], 'randomforest__max_depth': [6, 8, 10, 12, 14, 16, 18], 'randomforest__n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190], 'randomforest__random_state': [1]}",[0.52088452 0.84170854 0.19875776],"              precision    recall  f1-score   support

        news       0.46      0.60      0.52       177
     opinion       0.94      0.76      0.84       878
        fake       0.12      0.64      0.20        25

    accuracy                           0.73      1080
   macro avg       0.51      0.67      0.52      1080
weighted avg       0.84      0.73      0.77      1080
","{'randomforest__random_state': 1, 'randomforest__n_estimators': 140, 'randomforest__min_samples_leaf': 3, 'randomforest__max_features': 0.5000000000000001, 'randomforest__max_depth': 18}","{'randomforest__min_samples_leaf': [2, 3], 'randomforest__max_features': [0.4, 0.5], 'randomforest__max_depth': [16, 17, 18, 19], 'randomforest__n_estimators': [130, 135, 140, 145], 'randomforest__random_state': [1]}",[0.52088452 0.84223759 0.20987654],"              precision    recall  f1-score   support

        news       0.46      0.60      0.52       177
     opinion       0.94      0.76      0.84       877
        fake       0.12      0.65      0.21        26

    accuracy                           0.73      1080
   macro avg       0.51      0.67      0.52      1080
weighted avg       0.84      0.73      0.77      1080
","{'randomforest__max_depth': 18, 'randomforest__max_features': 0.5, 'randomforest__min_samples_leaf': 3, 'randomforest__n_estimators': 145, 'randomforest__random_state': 1}"
randomforest,True,True,"{'randomforest__min_samples_leaf': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59], 'randomforest__max_features': [0.1, 0.30000000000000004, 0.5000000000000001, 0.7000000000000001, 0.9000000000000001], 'randomforest__max_depth': [6, 8, 10, 12, 14, 16, 18], 'randomforest__n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190], 'randomforest__random_state': [1]}",[0.54135338 0.8103321  0.30769231],"              precision    recall  f1-score   support

        news       0.63      0.48      0.54       302
     opinion       0.77      0.86      0.81       641
        fake       0.31      0.31      0.31       137

    accuracy                           0.68      1080
   macro avg       0.57      0.55      0.55      1080
weighted avg       0.67      0.68      0.67      1080
","{'randomforest__random_state': 1, 'randomforest__n_estimators': 190, 'randomforest__min_samples_leaf': 3, 'randomforest__max_features': 0.1, 'randomforest__max_depth': 18}","{'randomforest__min_samples_leaf': [2, 3], 'randomforest__max_features': [0.1, 0.2], 'randomforest__max_depth': [16, 17, 18, 19], 'randomforest__n_estimators': [180, 185, 190, 195], 'randomforest__random_state': [1]}",[0.52091255 0.79912023 0.25185185],"              precision    recall  f1-score   support

        news       0.60      0.46      0.52       296
     opinion       0.76      0.84      0.80       650
        fake       0.25      0.25      0.25       134

    accuracy                           0.66      1080
   macro avg       0.54      0.52      0.52      1080
weighted avg       0.65      0.66      0.65      1080
","{'randomforest__max_depth': 17, 'randomforest__max_features': 0.2, 'randomforest__min_samples_leaf': 3, 'randomforest__n_estimators': 185, 'randomforest__random_state': 1}"
randomforest,True,False,"{'randomforest__min_samples_leaf': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59], 'randomforest__max_features': [0.1, 0.30000000000000004, 0.5000000000000001, 0.7000000000000001, 0.9000000000000001], 'randomforest__max_depth': [6, 8, 10, 12, 14, 16, 18], 'randomforest__n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190], 'randomforest__random_state': [1]}",[0.53228963 0.80029261 0.31205674],"              precision    recall  f1-score   support

        news       0.59      0.48      0.53       281
     opinion       0.77      0.84      0.80       653
        fake       0.32      0.30      0.31       146

    accuracy                           0.67      1080
   macro avg       0.56      0.54      0.55      1080
weighted avg       0.66      0.67      0.66      1080
","{'randomforest__random_state': 1, 'randomforest__n_estimators': 140, 'randomforest__min_samples_leaf': 3, 'randomforest__max_features': 0.5000000000000001, 'randomforest__max_depth': 18}","{'randomforest__min_samples_leaf': [2, 3], 'randomforest__max_features': [0.4, 0.5], 'randomforest__max_depth': [16, 17, 18, 19], 'randomforest__n_estimators': [130, 135, 140, 145], 'randomforest__random_state': [1]}",[0.54509804 0.80376539 0.29739777],"              precision    recall  f1-score   support

        news       0.60      0.50      0.55       280
     opinion       0.78      0.83      0.80       667
        fake       0.29      0.30      0.30       133

    accuracy                           0.68      1080
   macro avg       0.56      0.54      0.55      1080
weighted avg       0.67      0.68      0.67      1080
","{'randomforest__max_depth': 17, 'randomforest__max_features': 0.4, 'randomforest__min_samples_leaf': 3, 'randomforest__n_estimators': 130, 'randomforest__random_state': 1}"
xgboost,True,False,"{'xgboost__booster': ['gbtree', 'dart'], 'xgboost__tree_method': ['exact', 'hist', 'approx'], 'xgboost__max_depth': [3, 5, 7, 9, 11, 13, 15, 17, 19], 'xgboost__learning_rate': [0.01, 0.060000000000000005, 0.11, 0.16000000000000003, 0.21000000000000002, 0.26, 0.31000000000000005, 0.36000000000000004, 0.41000000000000003, 0.46], 'xgboost__gamma': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__reg_alpha': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__reg_lambda': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__min_child_weight': [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5], 'xgboost__subsample': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__colsample_bytree': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__colsample_bylevel': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__random_state': [1]}",[0.51599147 0.81571429 0.3024055 ],"              precision    recall  f1-score   support

        news       0.53      0.51      0.52       239
     opinion       0.80      0.83      0.82       686
        fake       0.32      0.28      0.30       155

    accuracy                           0.68      1080
   macro avg       0.55      0.54      0.54      1080
weighted avg       0.67      0.68      0.68      1080
","{'xgboost__tree_method': 'hist', 'xgboost__subsample': 0.5, 'xgboost__reg_lambda': 0.4, 'xgboost__reg_alpha': 0.4, 'xgboost__random_state': 1, 'xgboost__min_child_weight': 4.0, 'xgboost__max_depth': 15, 'xgboost__learning_rate': 0.060000000000000005, 'xgboost__gamma': 0.4, 'xgboost__colsample_bytree': 0.7, 'xgboost__colsample_bylevel': 0.6, 'xgboost__booster': 'gbtree'}","{'xgboost__tree_method': ['hist'], 'xgboost__subsample': [0.4, 0.5], 'xgboost__reg_lambda': [0.3, 0.4], 'xgboost__reg_alpha': [0.3, 0.4], 'xgboost__random_state': [1], 'xgboost__min_child_weight': [3.5, 4.0], 'xgboost__max_depth': [14, 15], 'xgboost__learning_rate': [0.059], 'xgboost__gamma': [0.3, 0.4], 'xgboost__colsample_bytree': [0.6, 0.7, 0.7999999999999999], 'xgboost__colsample_bylevel': [0.5, 0.6], 'xgboost__booster': ['gbtree']}",[0.5375     0.81539558 0.3032491 ],"              precision    recall  f1-score   support

        news       0.56      0.52      0.54       250
     opinion       0.80      0.83      0.82       689
        fake       0.31      0.30      0.30       141

    accuracy                           0.69      1080
   macro avg       0.56      0.55      0.55      1080
weighted avg       0.68      0.69      0.68      1080
","{'xgboost__booster': 'gbtree', 'xgboost__colsample_bylevel': 0.6, 'xgboost__colsample_bytree': 0.6, 'xgboost__gamma': 0.4, 'xgboost__learning_rate': 0.059, 'xgboost__max_depth': 14, 'xgboost__min_child_weight': 3.5, 'xgboost__random_state': 1, 'xgboost__reg_alpha': 0.3, 'xgboost__reg_lambda': 0.3, 'xgboost__subsample': 0.5, 'xgboost__tree_method': 'hist'}"
xgboost,False,False,"{'xgboost__booster': ['gbtree', 'dart'], 'xgboost__tree_method': ['exact', 'hist', 'approx'], 'xgboost__max_depth': [3, 5, 7, 9, 11, 13, 15, 17, 19], 'xgboost__learning_rate': [0.01, 0.060000000000000005, 0.11, 0.16000000000000003, 0.21000000000000002, 0.26, 0.31000000000000005, 0.36000000000000004, 0.41000000000000003, 0.46], 'xgboost__gamma': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__reg_alpha': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__reg_lambda': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__min_child_weight': [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5], 'xgboost__subsample': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__colsample_bytree': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__colsample_bylevel': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__random_state': [1]}",[0.47979798 0.82939297 0.23115578],"              precision    recall  f1-score   support

        news       0.41      0.57      0.48       166
     opinion       0.91      0.76      0.83       851
        fake       0.17      0.37      0.23        63

    accuracy                           0.71      1080
   macro avg       0.50      0.57      0.51      1080
weighted avg       0.79      0.71      0.74      1080
","{'xgboost__tree_method': 'hist', 'xgboost__subsample': 0.7, 'xgboost__reg_lambda': 0.4, 'xgboost__reg_alpha': 0.6000000000000001, 'xgboost__random_state': 1, 'xgboost__min_child_weight': 4.0, 'xgboost__max_depth': 19, 'xgboost__learning_rate': 0.21000000000000002, 'xgboost__gamma': 0.6000000000000001, 'xgboost__colsample_bytree': 0.6, 'xgboost__colsample_bylevel': 0.7, 'xgboost__booster': 'gbtree'}","{'xgboost__tree_method': ['hist'], 'xgboost__subsample': [0.6, 0.7, 0.7999999999999999], 'xgboost__reg_lambda': [0.3, 0.4], 'xgboost__reg_alpha': [0.5, 0.6], 'xgboost__random_state': [1], 'xgboost__min_child_weight': [3.5, 4.0], 'xgboost__max_depth': [18, 19], 'xgboost__learning_rate': [0.2, 0.21000000000000002], 'xgboost__gamma': [0.5, 0.6], 'xgboost__colsample_bytree': [0.5, 0.6], 'xgboost__colsample_bylevel': [0.6, 0.7, 0.7999999999999999], 'xgboost__booster': ['gbtree']}",[0.51084337 0.83301344 0.24175824],"              precision    recall  f1-score   support

        news       0.46      0.57      0.51       185
     opinion       0.91      0.77      0.83       849
        fake       0.16      0.48      0.24        46

    accuracy                           0.72      1080
   macro avg       0.51      0.61      0.53      1080
weighted avg       0.80      0.72      0.75      1080
","{'xgboost__booster': 'gbtree', 'xgboost__colsample_bylevel': 0.7, 'xgboost__colsample_bytree': 0.5, 'xgboost__gamma': 0.5, 'xgboost__learning_rate': 0.21000000000000002, 'xgboost__max_depth': 18, 'xgboost__min_child_weight': 4.0, 'xgboost__random_state': 1, 'xgboost__reg_alpha': 0.5, 'xgboost__reg_lambda': 0.4, 'xgboost__subsample': 0.6, 'xgboost__tree_method': 'hist'}"
xgboost,True,True,"{'xgboost__booster': ['gbtree', 'dart'], 'xgboost__tree_method': ['exact', 'hist', 'approx'], 'xgboost__max_depth': [3, 5, 7, 9, 11, 13, 15, 17, 19], 'xgboost__learning_rate': [0.01, 0.060000000000000005, 0.11, 0.16000000000000003, 0.21000000000000002, 0.26, 0.31000000000000005, 0.36000000000000004, 0.41000000000000003, 0.46], 'xgboost__gamma': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__reg_alpha': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__reg_lambda': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__min_child_weight': [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5], 'xgboost__subsample': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__colsample_bytree': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__colsample_bylevel': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__random_state': [1]}",[0.52173913 0.81748786 0.30888031],"              precision    recall  f1-score   support

        news       0.52      0.52      0.52       230
     opinion       0.82      0.81      0.82       727
        fake       0.29      0.33      0.31       123

    accuracy                           0.69      1080
   macro avg       0.55      0.55      0.55      1080
weighted avg       0.70      0.69      0.70      1080
","{'xgboost__tree_method': 'hist', 'xgboost__subsample': 0.5, 'xgboost__reg_lambda': 0.2, 'xgboost__reg_alpha': 0.6000000000000001, 'xgboost__random_state': 1, 'xgboost__min_child_weight': 1.5, 'xgboost__max_depth': 19, 'xgboost__learning_rate': 0.11, 'xgboost__gamma': 0.2, 'xgboost__colsample_bytree': 0.8999999999999999, 'xgboost__colsample_bylevel': 0.7999999999999999, 'xgboost__booster': 'gbtree'}","{'xgboost__tree_method': ['hist'], 'xgboost__subsample': [0.4, 0.5], 'xgboost__reg_lambda': [0.1, 0.2], 'xgboost__reg_alpha': [0.5, 0.6], 'xgboost__random_state': [1], 'xgboost__min_child_weight': [1.0, 1.5], 'xgboost__max_depth': [18, 19], 'xgboost__learning_rate': [0.1, 0.11], 'xgboost__gamma': [0.1, 0.2], 'xgboost__colsample_bytree': [0.8, 0.9], 'xgboost__colsample_bylevel': [0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__booster': ['gbtree']}",[0.53215078 0.81686247 0.29770992],"              precision    recall  f1-score   support

        news       0.52      0.54      0.53       221
     opinion       0.83      0.81      0.82       733
        fake       0.29      0.31      0.30       126

    accuracy                           0.69      1080
   macro avg       0.55      0.55      0.55      1080
weighted avg       0.70      0.69      0.70      1080
","{'xgboost__booster': 'gbtree', 'xgboost__colsample_bylevel': 0.8999999999999999, 'xgboost__colsample_bytree': 0.8, 'xgboost__gamma': 0.1, 'xgboost__learning_rate': 0.11, 'xgboost__max_depth': 19, 'xgboost__min_child_weight': 1.5, 'xgboost__random_state': 1, 'xgboost__reg_alpha': 0.5, 'xgboost__reg_lambda': 0.2, 'xgboost__subsample': 0.5, 'xgboost__tree_method': 'hist'}"
xgboost,False,True,"{'xgboost__booster': ['gbtree', 'dart'], 'xgboost__tree_method': ['exact', 'hist', 'approx'], 'xgboost__max_depth': [3, 5, 7, 9, 11, 13, 15, 17, 19], 'xgboost__learning_rate': [0.01, 0.060000000000000005, 0.11, 0.16000000000000003, 0.21000000000000002, 0.26, 0.31000000000000005, 0.36000000000000004, 0.41000000000000003, 0.46], 'xgboost__gamma': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__reg_alpha': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__reg_lambda': [0.0, 0.2, 0.4, 0.6000000000000001, 0.8], 'xgboost__min_child_weight': [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5], 'xgboost__subsample': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__colsample_bytree': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__colsample_bylevel': [0.5, 0.6, 0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__random_state': [1]}",[0.45192308 0.69449082 0.24852071],"              precision    recall  f1-score   support

        news       0.61      0.36      0.45       394
     opinion       0.58      0.86      0.69       484
        fake       0.31      0.21      0.25       202

    accuracy                           0.55      1080
   macro avg       0.50      0.48      0.46      1080
weighted avg       0.54      0.55      0.52      1080
","{'xgboost__tree_method': 'hist', 'xgboost__subsample': 0.7999999999999999, 'xgboost__reg_lambda': 0.6000000000000001, 'xgboost__reg_alpha': 0.8, 'xgboost__random_state': 1, 'xgboost__min_child_weight': 4.5, 'xgboost__max_depth': 3, 'xgboost__learning_rate': 0.26, 'xgboost__gamma': 0.8, 'xgboost__colsample_bytree': 0.8999999999999999, 'xgboost__colsample_bylevel': 0.8999999999999999, 'xgboost__booster': 'dart'}","{'xgboost__tree_method': ['hist'], 'xgboost__subsample': [0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__reg_lambda': [0.5, 0.6], 'xgboost__reg_alpha': [0.7, 0.7999999999999999, 0.8999999999999999], 'xgboost__random_state': [1], 'xgboost__min_child_weight': [4.0, 4.5], 'xgboost__max_depth': [2, 3], 'xgboost__learning_rate': [0.25, 0.26, 0.27], 'xgboost__gamma': [0.7, 0.7999999999999999], 'xgboost__colsample_bytree': [0.8, 0.9], 'xgboost__colsample_bylevel': [0.8, 0.9], 'xgboost__booster': ['dart']}",[0.47384615 0.7120332  0.28196721],"              precision    recall  f1-score   support

        news       0.67      0.37      0.47       420
     opinion       0.60      0.87      0.71       491
        fake       0.32      0.25      0.28       169

    accuracy                           0.58      1080
   macro avg       0.53      0.50      0.49      1080
weighted avg       0.58      0.58      0.55      1080
","{'xgboost__booster': 'dart', 'xgboost__colsample_bylevel': 0.8, 'xgboost__colsample_bytree': 0.8, 'xgboost__gamma': 0.7, 'xgboost__learning_rate': 0.25, 'xgboost__max_depth': 2, 'xgboost__min_child_weight': 4.0, 'xgboost__random_state': 1, 'xgboost__reg_alpha': 0.7, 'xgboost__reg_lambda': 0.5, 'xgboost__subsample': 0.8999999999999999, 'xgboost__tree_method': 'hist'}"
