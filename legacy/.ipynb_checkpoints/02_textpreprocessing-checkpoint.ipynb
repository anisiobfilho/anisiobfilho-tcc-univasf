{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f63f0-6d84-44e1-99db-ee38b2b6bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCC: \n",
    "# Code: \n",
    "# Author: Anísio Pereira Batista Filho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a36f33-f3e1-4523-bce2-1626119093cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Essentials\n",
    "import os\n",
    "import csv\n",
    "import numpy as np ##Numpy\n",
    "import pandas as pd ##Pandas\n",
    "##Matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "##Ekphrasis\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "##NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "##Cogroo4py\n",
    "#from utils.cogroo4py.cogroo_interface import cogroo\n",
    "##Cogroo-Interface\n",
    "#from cogroo_interface import Cogroo\n",
    "#cogroo = Cogroo.Instance()\n",
    "from cogroo4py.cogroo import Cogroo\n",
    "cogroo = Cogroo()\n",
    "##Spellchecker\n",
    "from spellchecker import SpellChecker\n",
    "##Wordcloud\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "##Utils\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d8493-06a7-4deb-8e30-6e3184a9034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcd743-bf26-4900-bff1-291db87a65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Spell checker:\n",
    "def spellchecker_pt(df, rowname):\n",
    "    print('-Início do spellchecker_pt-')\n",
    "    spell = SpellChecker(language='pt')\n",
    "    correction_list = []\n",
    "    for index, row in tqdm(df.iterrows(),total=df.shape[0]):\n",
    "        correction = spell.correction(row[rowname])\n",
    "        correction_list.append(correction)\n",
    "\n",
    "    return correction_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec102e-bd84-4ca3-b0bc-fbdc283312ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converter caracteres de tweet_text para 'lower case':\n",
    "def lowercase_converter(df, rowname):\n",
    "    print('-Início do lowercase_Converter-')\n",
    "    tweet_text_lower = []\n",
    "    for index, row in tqdm(df.iterrows(),total=df.shape[0]):\n",
    "        lower_row = row[rowname].lower()\n",
    "        tweet_text_lower.append(lower_row)\n",
    "\n",
    "    return tweet_text_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c2edd-d0ea-43bd-a791-dce912144c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-processamento de texto com Ekphrasis:\n",
    "def preproc_ekphrasis(df, rowname):\n",
    "    print('-Início do preproc_ekphrasis-')\n",
    "    text_processor = TextPreProcessor(\n",
    "        # terms that will be normalized\n",
    "        normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "            'time', 'url', 'date', 'number'],\n",
    "        # terms that will be annotated\n",
    "        annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "            'emphasis', 'censored'},\n",
    "        fix_html=True,  # fix HTML tokens\n",
    "        \n",
    "        # corpus from which the word statistics are going to be used \n",
    "        # for word segmentation \n",
    "        segmenter=\"twitter\", \n",
    "        \n",
    "        # corpus from which the word statistics are going to be used \n",
    "        # for spell correction\n",
    "        corrector=\"twitter\", \n",
    "        \n",
    "        unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "        unpack_contractions=False,  # Unpack contractions (can't -> can not)\n",
    "        spell_correct_elong=False,  # spell correction for elongated words\n",
    "        \n",
    "        # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "        # the tokenizer, should take as input a string and return a list of tokens\n",
    "        tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "        \n",
    "        # list of dictionaries, for replacing tokens extracted from the text,\n",
    "        # with other expressions. You can pass more than one dictionaries.\n",
    "        dicts=[emoticons]\n",
    "    )\n",
    "\n",
    "    preproc_list = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        line = row[rowname]\n",
    "        #print(\" \".join(text_processor.pre_process_doc(line)))\n",
    "        preproc_list.append(\" \".join(text_processor.pre_process_doc(line)))\n",
    "        #print(preproc_list)\n",
    "\n",
    "    return preproc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e997fb51-026b-4394-b13e-fbe1bd0b38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remoção de Stopwords utilizando NLTK:\n",
    "def stopwords_ntlk(preproc_list = []):\n",
    "    print('-Início do stopwords_nltk-')\n",
    "    stopWords = nltk.corpus.stopwords.words('portuguese')\n",
    "    stopwords_list = []\n",
    "    for line in tqdm(preproc_list, total=len(preproc_list)):\n",
    "        words = word_tokenize(line, language='portuguese')\n",
    "        sent = []\n",
    "        for w in words:\n",
    "            if w not in stopWords:\n",
    "                sent.append(w)\n",
    "        sent_str = \" \".join(sent)\n",
    "        stopwords_list.append(sent_str)\n",
    "\n",
    "    #print(stopwords_list)\n",
    "    return stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018d32a-39f8-4214-a625-fbf614dd0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remoção de símbolos:\n",
    "def symbol_remover(stopwords_list = []):\n",
    "    print('-Início do symbol_remover-')\n",
    "    symboless_list = []\n",
    "    for line in tqdm(stopwords_list, total=len(stopwords_list)):\n",
    "        # Unicode normalize transforma um caracter em seu equivalente em latin.\n",
    "        nfkd = unicodedata.normalize('NFKD', line)\n",
    "        new_line = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "        # Usa expressão regular para retornar a palavra apenas com números, letras e espaço\n",
    "        symboless_line = re.sub('[^a-zA-Z0-9 \\\\\\\\]', ' ', new_line)\n",
    "        symboless_line = \" \".join(symboless_line.split())\n",
    "        if symboless_line.strip():\n",
    "            symboless_list.append(symboless_line)\n",
    "        #print(line)\n",
    "\n",
    "    return symboless_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19525381-88d1-4d71-8afb-e445bfa1a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Substituição de palavras:\n",
    "def wordreplacer(symboless_list = []):\n",
    "    print('-Início do wordreplacer-')\n",
    "    ###Abertura de arquivos e criação de dataframes:\n",
    "    abreviações_internet_file = pd.read_csv(\"data/utils/abreviações_internet.csv\")\n",
    "    abreviações_estados_file = pd.read_csv(\"data/utils/abreviações_estados.csv\")\n",
    "    flexões_vacina_file = pd.read_csv(\"data/utils/flexões_vacinar.csv\")\n",
    "    df_internet = pd.DataFrame(abreviações_internet_file)\n",
    "    df_estados = pd.DataFrame(abreviações_estados_file)\n",
    "    df_vacinar = pd.DataFrame(flexões_vacina_file)\n",
    "\n",
    "    ###Criação de listas de substituição:\n",
    "    covidReplace = [ 'covid', 'covid-19', 'covid19', 'coronavirus', 'corona', 'virus' ]\n",
    "    vacinaReplace = [ 'coronavac', 'astrazeneca', 'pfizer', 'sputnik v', 'sputnik', \n",
    "                    'sinovac', 'oxford', 'moderna', 'butantan', 'johnson', \n",
    "                    'johnson&johnson', 'jnj', 'fio cruz', 'fiocruz' \n",
    "                    ]\n",
    "    ab_internet = dict()\n",
    "    for row in tqdm(df_internet.itertuples(), total=df_internet.shape[0]):\n",
    "        ab_internet[row.sigla] = row.significado\n",
    "\n",
    "    ab_estados = dict()\n",
    "    for row in tqdm(df_estados.itertuples(), total=df_estados.shape[0]):\n",
    "        ab_estados[row.sigla] = row.estado\n",
    "\n",
    "    ab_vacinar = []\n",
    "    for row in tqdm(df_vacinar.itertuples(), total=df_vacinar.shape[0]):\n",
    "        ab_vacinar.append(row[0])\n",
    "\n",
    "    ###Substituindo palavras:\n",
    "    wordreplaced_list = []\n",
    "    for line in tqdm(symboless_list, total=len(symboless_list)):\n",
    "        words = word_tokenize(line, language='portuguese')\n",
    "        #line = \" \".join(line.split())\n",
    "        sent = []\n",
    "        for word in words:\n",
    "            if word in ab_internet:\n",
    "                word = word.replace(word, ab_internet[word])\n",
    "            if word in ab_estados:\n",
    "                word = word.replace(word, ab_estados[word])\n",
    "            if word in ab_vacinar:\n",
    "                word = word.replace(word, 'vacina')\n",
    "            if word in covidReplace:\n",
    "                word = word.replace(word, 'covid')            \n",
    "            if word in vacinaReplace:\n",
    "                word = word.replace(word, 'vacina')\n",
    "            sent.append(word)\n",
    "        #line = \" \".join(word)\n",
    "        sent_str = \" \".join(sent)\n",
    "        wordreplaced_list.append(sent_str)\n",
    "    #print (wordreplaced_list)\n",
    "    return wordreplaced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf89c44-e5bd-4e32-aebd-838a6aa79aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remoção de palavras:\n",
    "def wordremover(wordreplaced_list = []):\n",
    "    print('-Início do wordremover-')\n",
    "    ###Abertura de arquivo e criação de dataframe:\n",
    "    stopwords_internet_file = pd.read_csv(\"data/utils/stopwords_internet_symboless.csv\")\n",
    "    df_stopwords = pd.DataFrame(stopwords_internet_file)\n",
    "\n",
    "    ###Criação da lista de stopwords:\n",
    "    sw_internet = []\n",
    "    for row in tqdm(df_stopwords.itertuples(), total=df_stopwords.shape[0]):\n",
    "        sw = row.stopwords\n",
    "        sw = \"\".join(re.split(r\"\\s+\", sw))\n",
    "        sw_internet.append(sw)\n",
    "\n",
    "    ###Removendo palavras:\n",
    "    wordremoved_list = []\n",
    "    for line in tqdm(wordreplaced_list, total=len(wordreplaced_list)):\n",
    "        words = word_tokenize(line, language='portuguese')\n",
    "        #line = \" \".join(line.split())\n",
    "        sent = []\n",
    "        for word in words:\n",
    "            if word not in sw_internet:\n",
    "                sent.append(word)\n",
    "        sent_str = \" \".join(sent)\n",
    "        wordremoved_list.append(sent_str)\n",
    "    #print(wordremoved_list)\n",
    "    return wordremoved_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca49001-bfda-4621-a298-add36fbac75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Stemming utilizando NLTK:\n",
    "def stemming_nltk(wordremoved_list = []):\n",
    "    print('-Início do stemming_nltk-')\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    stemmed_list = []\n",
    "    for line in tqdm(wordremoved_list, total=len(wordremoved_list)):\n",
    "        #line = \" \".join(line.split())\n",
    "        words = word_tokenize(line, language='portuguese')\n",
    "        sent = []\n",
    "        for word in words:\n",
    "            sent.append(stemmer.stem(word))\n",
    "        sent_str = \" \".join(sent)\n",
    "        stemmed_list.append(sent_str)\n",
    "\n",
    "    return stemmed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fd504-364d-47cb-859e-99908de17b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lemmatization utilizando Cogroo4py:\n",
    "def lemmatization_cogroo4py(wordremoved_list = []):\n",
    "    print('-Início do lemmatization_cogroo4py-')\n",
    "    lemmatized_list = []\n",
    "    for line in tqdm(wordremoved_list, total=len(wordremoved_list)):\n",
    "        #line = \" \".join(line.split())\n",
    "        #line = cogroo.lemmatize(line)\n",
    "        #line = line.lower()\n",
    "        words = word_tokenize(line, language='portuguese')\n",
    "        sent = []\n",
    "        for word in words:\n",
    "            sent.append(cogroo.lemmatize(word).lower())\n",
    "        sent_str = \" \".join(sent)\n",
    "        lemmatized_list.append(sent_str)\n",
    "\n",
    "    return lemmatized_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f237f3-0ab5-400f-8af2-284995d4abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Contador de frequência de palavras:\n",
    "def wordfrequency(word_list= []):\n",
    "    print('-Início do wordfrequency-')\n",
    "    wordstring = \" \".join(word_list)\n",
    "    wordlist = wordstring.split()\n",
    "    wordfreq = []\n",
    "    wordfreq = [wordlist.count(p) for p in tqdm(wordlist, total=len(wordlist))]\n",
    "    freqdict = dict(list(zip(wordlist,wordfreq)))\n",
    "    sorteddict = dict(sorted(freqdict.items(), key=lambda t: t[1], reverse=True))\n",
    "\n",
    "    frequency_list = []\n",
    "    for s in tqdm(sorteddict.items(), total=len(sorteddict)): \n",
    "        #print(str(s))\n",
    "        frequency_list.append(str(s))\n",
    "\n",
    "    return frequency_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb9e98-b5dd-479f-ae69-48e4bd812cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Criação de wordcloud:\n",
    "def wordcloud_generator(word_list = []):\n",
    "    print('-Início do wordcloud_generator-')\n",
    "    all_summary = \" \".join(word_list)\n",
    "    wordcloud = WordCloud(\n",
    "                      background_color='black', \n",
    "                      width=1600,                            \n",
    "                      height=800).generate(all_summary)\n",
    "    fig, ax = plt.subplots(figsize=(16,8))\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')       \n",
    "    ax.set_axis_off()\n",
    "    plt.imshow(wordcloud) \n",
    "    \n",
    "    return wordcloud                \n",
    "    #wordcloud.to_file('testcloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b793685-490e-47ed-b77b-e75eac2c9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_original(df):\n",
    "    ####Pré-processamento de texto e criação de colunas:\n",
    "    lower_list = lowercase_converter(df, 'tweet_text')\n",
    "    try:\n",
    "        df.insert(4, 'tweet_text_lower', lower_list)\n",
    "    except:\n",
    "        df['tweet_text_lower'] = lower_list\n",
    "    preproc_list = preproc_ekphrasis(df, 'tweet_text_lower')\n",
    "    stopwords_list = stopwords_ntlk(preproc_list)\n",
    "    symboless_list = symbol_remover(stopwords_list)\n",
    "    wordreplaced_list = wordreplacer(symboless_list)\n",
    "    wordremoved_list = wordremover(wordreplaced_list)\n",
    "    stemmed_list = stemming_nltk(wordremoved_list)\n",
    "    try:\n",
    "        df.insert(5, 'tweet_text_stemmed', stemmed_list)\n",
    "    except:\n",
    "        df['tweet_text_stemmed'] = stemmed_list\n",
    "    lemmatized_list = lemmatization_cogroo4py(wordremoved_list)\n",
    "    try:\n",
    "        df.insert(6, 'tweet_text_lemmatized', lemmatized_list)\n",
    "    except:\n",
    "        df['tweet_text_lemmatized'] = lemmatized_list\n",
    "    stemmed_frequency = wordfrequency(stemmed_list)\n",
    "    lemmatized_frequency = wordfrequency(lemmatized_list)\n",
    "\n",
    "    ####Geração de wordclouds:\n",
    "    stemmed_cloud = wordcloud_generator(stemmed_frequency)\n",
    "    stemmed_cloud.to_file(\"data/corpus_labeled/iguais/rotulacao_uniao[iguais]_stemmcloud.png\")\n",
    "    lemmatized_cloud = wordcloud_generator(lemmatized_frequency)\n",
    "    lemmatized_cloud.to_file(\"wordcloud/corpus_labeled/iguais/rotulacao_uniao[iguais]_lemmacloud.png\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890b85c-4125-4d42-9cd8-9d9bb83aa1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_spellchecked(df):\n",
    "    spellchecked_list = spellchecker_pt(df, 'tweet_text')\n",
    "    try:\n",
    "        df.insert(4, 'tweet_text_spellchecked', spellchecked_list)\n",
    "    except:\n",
    "        df['tweet_text_spellchecked'] = spellchecked_list\n",
    "    lower_list = lowercase_converter(df, 'tweet_text_spellchecked')\n",
    "    try:\n",
    "        df.insert(5 'tweet_text_spellchecked_lower', lower_list)\n",
    "    except:\n",
    "        df['tweet_text_spellchecked_lower'] = lower_list\n",
    "    preproc_list = preproc_ekphrasis(df, 'tweet_text_spellchecked_lower')\n",
    "    stopwords_list = stopwords_ntlk(preproc_list)\n",
    "    symboless_list = symbol_remover(stopwords_list)\n",
    "    wordreplaced_list = wordreplacer(symboless_list)\n",
    "    wordremoved_list = wordremover(wordreplaced_list)\n",
    "    stemmed_list = stemming_nltk(wordremoved_list)\n",
    "    try:\n",
    "        df.insert(6 'tweet_text_spellchecked_stemmed', stemmed_list)\n",
    "    except:\n",
    "        df['tweet_text_spellchecked_stemmed'] = stemmed_list\n",
    "    lemmatized_list = lemmatization_cogroo4py(wordremoved_list)\n",
    "    try:\n",
    "        df.insert(7 'tweet_text_spellchecked_lemmatized', lemmatized_list)\n",
    "    except:\n",
    "        df['tweet_text_spellchecked_lemmatized'] = lemmatized_list\n",
    "    stemmed_frequency = wordfrequency(stemmed_list)\n",
    "    lemmatized_frequency = wordfrequency(lemmatized_list)\n",
    "\n",
    "    ####Geração de wordclouds:\n",
    "    stemmed_cloud = wordcloud_generator(stemmed_frequency)\n",
    "    stemmed_cloud.to_file(\"wordcloud/corpus_labeled/iguais/rotulacao_uniao[iguais]_spellchecked_stemmcloud.png\")\n",
    "    lemmatized_cloud = wordcloud_generator(lemmatized_frequency)\n",
    "    lemmatized_cloud.to_file(\"wordcloud/corpus_labeled/iguais/rotulacao_uniao[iguais]_spellchecked_lemmacloud.png\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59333c-1d61-45fa-a5da-443f46fbcbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Função main:\n",
    "###Abertura de arquivo e criação do dataframe:\n",
    "#df_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[iguais].csv\", sep=\",\")\n",
    "#df = pd.DataFrame(df_file)\n",
    "#df = df.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "df_file = pd.read_csv(\"data/corpus_labeled/iguais/rotulacao_uniao[iguais].csv\", sep=\",\")\n",
    "df = pd.DataFrame(df_file)\n",
    "#df = df.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "###Chamadas de funções:\n",
    "####Pré-processamento de texto e criação de colunas:\n",
    "#preproc_original(df)\n",
    "preproc_spellchecked(df)\n",
    "\n",
    "###Salvando alterações no csv:\n",
    "add_file = df.to_csv(\"data/data-twitter/training/rotulaçao_uniao[iguais]_preproc.csv\", sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4bc8e-bf0f-4478-96be-99de64f6dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = timeit.default_timer()\n",
    "print ('Duração: %f' % (end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
