{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8348f5cf-b520-4c5c-9582-b4ca155977c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCC: \n",
    "# Code: \n",
    "# Author: Anísio Pereira Batista Filho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7878d4-82c1-4027-aeec-02a0e50661ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Essentials\n",
    "import os\n",
    "import csv\n",
    "import numpy as np ##Numpy\n",
    "import pandas as pd ##Pandas\n",
    "##Matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "##Ekphrasis\n",
    "#from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "#from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "#from ekphrasis.dicts.emoticons import emoticons\n",
    "##NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "##Cogroo4py\n",
    "#from utils.cogroo4py.cogroo_interface import cogroo\n",
    "##Wordcloud\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "##Tweepy\n",
    "import tweepy as tw\n",
    "##Geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "##Utils\n",
    "import re\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a863f3ab-015c-420b-972e-5d2cca60bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e5897c-6510-44e1-9175-f611c9fb9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Gerar dataframe de amostras:\n",
    "def df_get_samples(df, qnt):\n",
    "    df_sample = df.sample(n=qnt)\n",
    "    return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb300af-8219-414d-9489-e08670394029",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Eliminar duplicatas em um dataframe:\n",
    "def df_drop_duplicates(df):\n",
    "    df = df.drop_duplicates(subset='tweet_id', keep='first')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a69d707-e051-43e4-b7d3-f5c8e0632a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Comparador de treinamentos (compara dois treinamentos e cria um conjunto de iguais e um de diferentes)\n",
    "def compare_trainings(dfA, dfB):\n",
    "    listC = []\n",
    "    listD = []\n",
    "    for a in tqdm(dfA.itertuples(), total=dfA.shape[0]):\n",
    "        for b in dfB.itertuples():\n",
    "            if a.tweet_id == b.tweet_id:\n",
    "                if a.label == b.label:\n",
    "                    dictC = dict({'tweet_id':a.tweet_id,'created_at':a.created_at,'user_location':a.user_location,'tweet_text':a.tweet_text,'label_A':a.label,'label_B':b.label,'pct_certainty_A':a.pct_certainty,'pct_certainty_B':b.pct_certainty})\n",
    "                    listC.append(dictC)\n",
    "                else:\n",
    "                    dictD = dict({'tweet_id':a.tweet_id,'created_at':a.created_at,'user_location':a.user_location,'tweet_text':a.tweet_text,'label_A':a.label,'label_B':b.label,'pct_certainty_A':a.pct_certainty,'pct_certainty_B':b.pct_certainty})\n",
    "                    listD.append(dictD)     \n",
    "\n",
    "    dfC = pd.DataFrame(listC, columns=['tweet_id','created_at','user_location','tweet_text','label_A','label_B','pct_certainty_A','pct_certainty_B'])\n",
    "    dfD = pd.DataFrame(listD, columns=['tweet_id','created_at','user_location','tweet_text','label_A','label_B','pct_certainty_A','pct_certainty_B'])\n",
    "\n",
    "    dfC.to_csv(\"data/data-twitter/training/rotulaçao[iguais]2.csv\", sep=\",\", index=False)\n",
    "    dfD.to_csv(\"data/data-twitter/training/rotulaçao[diferentes]2.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ef7cd1-0001-4960-ae49-45a64e2476b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculo de Cohen Kappa:\n",
    "def cohen_kappa(df, dfA, dfB):\n",
    "    ##Criando dicionário de rótuloxquantidade com as rotulações do avaliador A:\n",
    "    labelerA = dict()\n",
    "    fake_news = opinion = news = 0\n",
    "    for row in dfA.itertuples():\n",
    "        if row.label == 1:\n",
    "            fake_news += 1\n",
    "            labelerA['fake_news'] = fake_news\n",
    "        else: \n",
    "            if row.label == 0:\n",
    "                opinion += 1\n",
    "                labelerA['opinion'] = opinion\n",
    "            else:\n",
    "                if row.label == -1:\n",
    "                    news += 1\n",
    "                    labelerA['news'] = news\n",
    "    print('Rotulador A: ',labelerA)\n",
    "\n",
    "    ##Calculando a probabilidade de concordancia randomica do avaliador A:\n",
    "    peA = dict()\n",
    "    peA['fake_news'] = labelerA['fake_news']/dfA.shape[0]\n",
    "    peA['opinion'] = labelerA['opinion']/dfA.shape[0]\n",
    "    peA['news'] = labelerA['news']/dfA.shape[0]\n",
    "    print('peA: ',peA)\n",
    "\n",
    "    ##Criando dicionário de rótuloxquantidade com as rotulações do avaliador B:\n",
    "    labelerB = dict()\n",
    "    fake_news = opinion = news = 0\n",
    "    for row in dfB.itertuples():\n",
    "        if row.label == 1:\n",
    "            fake_news += 1\n",
    "            labelerB['fake_news'] = fake_news\n",
    "        else: \n",
    "            if row.label == 0:\n",
    "                opinion += 1\n",
    "                labelerB['opinion'] = opinion\n",
    "            else:\n",
    "                if row.label == -1:\n",
    "                    news += 1\n",
    "                    labelerB['news'] = news\n",
    "    print('Rotulador B: ',labelerB)\n",
    "\n",
    "    ##Calculando a probabilidade de concordancia randomica do avaliador B:\n",
    "    peB = dict()\n",
    "    peB['fake_news'] = labelerB['fake_news']/dfB.shape[0]\n",
    "    peB['opinion'] = labelerB['opinion']/dfB.shape[0]\n",
    "    peB['news'] = labelerB['news']/dfB.shape[0]\n",
    "    print('peB: ',peB)\n",
    "\n",
    "    ##Calculando a probabilidade de ambos os avaliadores (para fake_news, opinion e news):\n",
    "    pe = dict()\n",
    "    pe['fake_news'] = peA['fake_news']*peB['fake_news']\n",
    "    pe['opinion'] = peA['opinion']*peB['opinion']\n",
    "    pe['news'] = peA['news']*peB['news']\n",
    "    print('pe:',pe)\n",
    "\n",
    "    ##Calculando a probabilidade de aceitação total:\n",
    "    Pr = pe['fake_news']+pe['opinion']+pe['news']\n",
    "    print('Pr: ',Pr)\n",
    "\n",
    "    ##Calculando Po\n",
    "    Po = df.shape[0]/dfA.shape[0]\n",
    "    print('Po: ',Po)\n",
    "    ##Calculando Kappa:\n",
    "    K = (Po - Pr)/(1 - Pr)\n",
    "    print('K: ',K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffaa03d7-e858-4c41-84d3-f5e59e334401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_interval(df):\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    dtidict = dict()\n",
    "    dtidict['Ínicio do intervalo'] = df['created_at'].min()\n",
    "    dtidict['Fim do invervalo'] = df['created_at'].max()\n",
    "    print(dtidict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f37fc3b-4d43-4a13-8613-6a52b0c8d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_labelbase(df_original, df_base4label, total):\n",
    "    #Criar uma lista com os ids utilizados na base anterior\n",
    "    used_tweetid_list = []\n",
    "    for row in df_base4label.itertuples():\n",
    "        used_tweetid_list.append(row.tweet_id)\n",
    "    #print(used_tweetid_list)\n",
    "    \n",
    "    #Calcular o tamanho da nova base desejada\n",
    "    qnt = total - df_base4label.shape[0]\n",
    "    #print(qnt)\n",
    "    #Criar o novo dataframe\n",
    "    df_newbase4label = pd.DataFrame()\n",
    "    #Atribuir as amostras (samples) ao novo dataframe\n",
    "    df_newbase4label = df_original.sample(n=qnt)\n",
    "    #df_newbase4label = df_newbase4label.set_index(\"tweet_id\")\n",
    "    print('Tamanho da base após pegar as primeiras samples: ', df_newbase4label.shape[0])\n",
    "\n",
    "    flag = 0\n",
    "    check = 0\n",
    "    while (flag != 1):\n",
    "        flag = 0\n",
    "        check = 0\n",
    "        print('-Começo de novo loop-')\n",
    "        #Verificar se existem duplicatas nas amostras, caso tenha elimina e pega novas amostras\n",
    "        #O loop termina quando não existem mais duplicatas no dataframe\n",
    "        while(check != 1):\n",
    "            print('-Dentro do while-')\n",
    "            #print('-Entrou no loop interno-')\n",
    "            df_newbase4label = df_newbase4label.drop_duplicates(subset='tweet_id', keep='first')\n",
    "            #df_newbase4label.reset_index().drop_duplicates(subset='tweet_id', keep='first').set_index('tweet_id')\n",
    "            #df_newbase4label = df_newbase4label[~df_newbase4label.index.duplicated(keep='first')]\n",
    "            print('Tamanho da base após dropar duplicatas: ', df_newbase4label.shape[0])\n",
    "            undropped = df_newbase4label.shape[0]\n",
    "            if undropped == qnt:\n",
    "                check = 1\n",
    "            else:\n",
    "                df_newbase4label = df_newbase4label.append(df_original.sample(n=(qnt-undropped)), ignore_index=True)\n",
    "                print('Tamanho da base após pegar novas samples após drop de duplicatas: ', df_newbase4label.shape[0])\n",
    "        print('-Saiu do while-')\n",
    "        print('-Começo do for-')\n",
    "        \n",
    "        #Com a lista de ids usados na primeira base para rotulação, ele verifica se no dataframe atual\n",
    "        #existe alguma repetição desse mesmo id, caso tenha, o id repetido é eliminado do novo dataframe\n",
    "        cont = 0\n",
    "        for ind in used_tweetid_list:\n",
    "            for row in df_newbase4label.itertuples():\n",
    "                if ind == row.tweet_id:\n",
    "                    #df_newbase4label = df_newbase4label.drop(ind, inplace=True)\n",
    "                #df_newbase4label = df_newbase4label.drop(df_newbase4label.loc[df_newbase4label['tweet_id']==ind].index, inplace=True)\n",
    "                    df_newbase4label = df_newbase4label.drop(df_newbase4label.index[df_newbase4label['tweet_id'] == ind])\n",
    "                    cont += 1\n",
    "        print('-Saiu do for-')\n",
    "        print('Contador: ', cont)\n",
    "        #Verificar tamanho da base após dropar duplicatas em relação a base original\n",
    "        print('Tamanho da base depois tirar as cópias da base antiga: ', df_newbase4label.shape[0])\n",
    "        #Caso o tamanho da base atual após os processos acimas sejam igual o tamanho desejado, \n",
    "        #ativa-se a flag para parar o loop principal\n",
    "        if df_newbase4label.shape[0] == qnt:\n",
    "            flag = 1\n",
    "        print('Flag: ', flag)\n",
    "        #df_newbase4label = df_newbase4label.set_index(\"tweet_id\")\n",
    "        df_newbase4label = df_newbase4label.append(df_original.sample(n=cont))\n",
    "        print('Tamanho da base após pegar novas samples: ', df_newbase4label.shape[0])\n",
    "    print('-Saiu do loop-')\n",
    "    print ('Tamanho da base final após sair do while: ', df_newbase4label.shape[0])\n",
    "    #print(df_newbase4label)\n",
    "    return df_newbase4label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e7f96a-1f8c-4fed-ac49-60380a1750af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Função main:\n",
    "###Abertura de arquivo e criação do dataframe:\n",
    "df_file = pd.read_csv(\"data/corpus_labeled/iguais/rotulacao_uniao[iguais].csv\", sep=\",\")\n",
    "\n",
    "#df_file = pd.read_csv(\"data/data-twitter/data-twitter-modfied-utf8.csv\", sep=\";\")#df_original = pd.DataFrame(df_file)\n",
    "#df_original = df_original.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "#df_base4label_file = pd.read_csv(\"data/data-twitter/base_para_rotulaçao.csv\", sep=\",\")\n",
    "#df_base4label = pd.DataFrame(df_base4label_file)\n",
    "#df_base4label = df_base4label.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "#df_labeliguais_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[iguais]_complete.csv\", sep=\";\")\n",
    "#df_labeliguais = pd.DataFrame(df_labeliguais_file)\n",
    "#df_labeliguais = df_labeliguais.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "#df_newbase4label_file = pd.read_csv(\"data/data-twitter/base_para_rotulaçao2.csv\", sep=\",\")\n",
    "#df_newbase4label = pd.DataFrame(df_newbase4label_file)\n",
    "#df_newbase4label = df_base4label.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "#df_file = pd.read_csv(\"data/data-twitter/data-twitter_upgraded.csv\", sep=\";\")\n",
    "#df = pd.DataFrame(df_file)\n",
    "#df = df.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "#df_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[iguais]_complete.csv\", sep=\";\")\n",
    "#df = pd.DataFrame(df_file)\n",
    "#df = df.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "#dfA_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[anisiofilho].csv\", sep=\",\")\n",
    "#dfA = pd.DataFrame(dfA_file)\n",
    "#dfA = dfA.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "#dfB_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[debora].csv\", sep=\",\")\n",
    "#dfB = pd.DataFrame(dfB_file)\n",
    "#dfB = dfB.drop_duplicates(subset='tweet_id', keep='first')\n",
    "\n",
    "#dfI_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[iguais].csv\", sep=\",\")\n",
    "#dfI = pd.DataFrame(dfI_file)\n",
    "\n",
    "#dfD_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[diferentes].csv\", sep=\",\")\n",
    "#dfD = pd.DataFrame(dfD_file)\n",
    "\n",
    "#dfI2_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[iguais]2.csv\", sep=\",\")\n",
    "#dfI2 = pd.DataFrame(dfI2_file)\n",
    "\n",
    "#dfD2_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[diferentes]2.csv\", sep=\",\")\n",
    "#dfD2 = pd.DataFrame(dfD2_file)\n",
    "\n",
    "#df_file = pd.read_csv(\"data/data-twitter/training/rotulaçao[iguais].csv\", sep=',')\n",
    "#df = pd.DataFrame(df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "937442c2-76c8-4011-9666-96ed90d14209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_location</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label_A</th>\n",
       "      <th>label_B</th>\n",
       "      <th>pct_certainty_A</th>\n",
       "      <th>pct_certainty_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1346556985861427201</td>\n",
       "      <td>2021-01-05 20:39:37+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@ingrid_maria57 @JairEuAcredito @gui_iah @clau...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>1349141617324814338</td>\n",
       "      <td>2021-01-12 23:50:02+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@mitags Chinês come até barata mas não quis a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1349118821098000385</td>\n",
       "      <td>2021-01-12 22:19:27+00:00</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>@RomuloSalgado4 @ChromusMaster @folha A África...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1349503179554172929</td>\n",
       "      <td>2021-01-13 23:46:45+00:00</td>\n",
       "      <td>Piracicaba, SP</td>\n",
       "      <td>@Daarknight_ pqp pega da pfizer ou da de oxford</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>1348734277983363072</td>\n",
       "      <td>2021-01-11 20:51:24+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@AllanorB Ele é só o animalzinho de estimação ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                created_at           user_location  \\\n",
       "1977  1346556985861427201 2021-01-05 20:39:37+00:00                     NaN   \n",
       "982   1349141617324814338 2021-01-12 23:50:02+00:00                     NaN   \n",
       "25    1349118821098000385 2021-01-12 22:19:27+00:00  Rio de Janeiro, Brasil   \n",
       "1919  1349503179554172929 2021-01-13 23:46:45+00:00          Piracicaba, SP   \n",
       "527   1348734277983363072 2021-01-11 20:51:24+00:00                     NaN   \n",
       "\n",
       "                                             tweet_text  label_A  label_B  \\\n",
       "1977  @ingrid_maria57 @JairEuAcredito @gui_iah @clau...        0      0.0   \n",
       "982   @mitags Chinês come até barata mas não quis a ...        1      1.0   \n",
       "25    @RomuloSalgado4 @ChromusMaster @folha A África...       -1     -1.0   \n",
       "1919    @Daarknight_ pqp pega da pfizer ou da de oxford        0      0.0   \n",
       "527   @AllanorB Ele é só o animalzinho de estimação ...        1      1.0   \n",
       "\n",
       "      pct_certainty_A  pct_certainty_B  \n",
       "1977              100            100.0  \n",
       "982                25            100.0  \n",
       "25                 25             75.0  \n",
       "1919              100            100.0  \n",
       "527                75            100.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df_file)\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0eb6ef0-db3d-4652-942a-db48ae020baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd39c08b-037d-4753-9077-2e5e0c034f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ínicio do intervalo': Timestamp('2021-01-05 06:12:09+0000', tz='UTC'), 'Fim do invervalo': Timestamp('2021-01-13 23:59:58+0000', tz='UTC')}\n"
     ]
    }
   ],
   "source": [
    "###Chamadas de funções:\n",
    "datetime_interval(df)\n",
    "#print('Tamanho da base original:', df_original.shape[0])\n",
    "#print('Tamanho da base rotulada:', df_base4label.shape[0])\n",
    "#print('Tamanho da base rotulada igual:', df_labeliguais.shape[0])\n",
    "#compare_trainings(dfA,dfB)\n",
    "#cohen_kappa(df, dfA, dfB)\n",
    "#print(dfI.shape[0])\n",
    "#print(dfD.shape[0])\n",
    "#print(dfI2.shape[0])\n",
    "#print(dfD2.shape[0])\n",
    "#dfnewbase4label = create_new_labelbase(df_original, df_base4label, 5000)\n",
    "\n",
    "###Salvando alterações no csv:\n",
    "#add_file = df.to_csv(\"data/data-twitter/training/rotulaçao[iguais]_complete2.csv\", sep=\";\", index=False)\n",
    "#add_file = dfnewbase4label.to_csv(\"data/data-twitter/base_para_rotulaçao2.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04372c7-e957-4ee0-b8b9-f488cc544735",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = timeit.default_timer()\n",
    "print ('Duração: %f' % (end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
